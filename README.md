# Transformer

PyTorch implementation of [Attention is All You Need](https://arxiv.org/abs/1706.03762) paper by Vaswani et al. This repository is mainly for me to learn more about NLP and to further my understanding of the underlying architecture of current LLMs.

Update 9/17: Added Rotary Position Enbedding (RoPE)

## References:
- Transformer
    - [Attention is All You Need](https://arxiv.org/abs/1706.03762)
    - [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
    - [The Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer/)
    - [Language Translation with nn.Transformer and torchtext](https://pytorch.org/tutorials/beginner/translation_transformer.html)
- RoPE
    - [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864)
    - [Rotary Embeddings: A Relative Revolution](https://blog.eleuther.ai/rotary-embeddings/)
    